{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9978819c-abbd-450b-a6cc-d049b96461d1",
   "metadata": {},
   "source": [
    "# Results for Generalization using KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b61c11e-58a4-483f-8926-db93ad6787f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# KL Divergence as suggested by the chatGPT usin gentropy\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f6e65c9-4392-4ffd-8a06-ec7e8220eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(path, output_files):\n",
    "    df_dict = {}\n",
    "    for i, file in enumerate(output_files):\n",
    "        df_dict[f'df_{i+1}'] = pd.read_csv(path+file)\n",
    "        df_dict[f'df_{i+1}'].columns = df_dict[f'df_{i+1}'].columns.astype(int)\n",
    "        \n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee4787a1-3107-47c9-9d49-7b0674674b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate KL divergence\n",
    "def kl_divergence(p, q, epsilon=1e-9):\n",
    "    return np.sum(p * np.log2(p / q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb4f5d-cd1e-4ea1-95e8-13eb06af2c30",
   "metadata": {},
   "source": [
    "# ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1e6251-6f7e-4d9e-83c1-629a8259b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get probability distribution files for weak generalization - Standard set transformer\n",
    "wg_pt_path1 = './strong_generalization/multi_class_probability_files/st/ModelNet40/Weak_Generalization/64_16_128_3_0.05/'\n",
    "wg_pt_output_files1 = [file for file in os.listdir(wg_pt_path1) if file.startswith('output_file_') and file.endswith('.csv')]\n",
    "\n",
    "#Get probability distribution files for strong generalization - Standard set transformer\n",
    "sg_pt_path1 = './strong_generalization/multi_class_probability_files/st/ModelNet40/Strong_Generalization/64_16_128_3_0.05_bs32_ss256_vss128/'\n",
    "sg_pt_output_files1 = [file for file in os.listdir(sg_pt_path1) if file.startswith('output_file_') and file.endswith('.csv')]\n",
    "\n",
    "#Get probability distribution files for weak generalization - contrastive pre-trained model\n",
    "wg_pt_path2 = './strong_generalization/multi_class_probability_files/st/ModelNet40/Weak_Generalization/masked_encoder_160_512_150_200_64_16_128_3_0.05_True_False_pd1024_ed64_lr0.001_sndl_ST/'\n",
    "wg_pt_output_files2 = [file for file in os.listdir(wg_pt_path2) if file.startswith('output_file_') and file.endswith('.csv')]\n",
    "\n",
    "#Get probability distribution files for strong generalization - contrastive pre-trained model\n",
    "sg_pt_path2 = './strong_generalization/multi_class_probability_files/st/ModelNet40/Strong_Generalization/masked_encoder_160_512_150_200_64_16_128_3_0.05_True_False_pd1024_ed64_lr0.001_sndl_ST_pd128_bs32_ss256_vss128/'\n",
    "sg_pt_output_files2 = [file for file in os.listdir(sg_pt_path2) if file.startswith('output_file_') and file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e4c64d-9feb-4ce7-8449-949051fb3da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_pt_df_dict1 = concat(wg_pt_path1, wg_pt_output_files1)\n",
    "sg_pt_df_dict1 = concat(sg_pt_path1, sg_pt_output_files1)\n",
    "wg_pt_df_dict2 = concat(wg_pt_path2, wg_pt_output_files2)\n",
    "sg_pt_df_dict2 = concat(sg_pt_path2, sg_pt_output_files2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8a8aa6e-f69a-4019-a681-55e6592ef0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D matrix from the DataFrames\n",
    "wg_pt_data_matrix1 = np.array([df1.values for df1 in wg_pt_df_dict1.values()])\n",
    "sg_pt_data_matrix1 = np.array([df1.values for df1 in sg_pt_df_dict1.values()])\n",
    "wg_pt_data_matrix2 = np.array([df1.values for df1 in wg_pt_df_dict2.values()])\n",
    "sg_pt_data_matrix2 = np.array([df1.values for df1 in sg_pt_df_dict2.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec4d8ee9-cd91-4be4-8bd7-16fe141fb033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 40, 40)\n",
      "(10, 40, 40)\n",
      "(10, 40, 40)\n",
      "(10, 40, 40)\n"
     ]
    }
   ],
   "source": [
    "print(wg_pt_data_matrix1.shape)\n",
    "print(sg_pt_data_matrix1.shape)\n",
    "print(wg_pt_data_matrix2.shape)\n",
    "print(sg_pt_data_matrix2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2bc9dfb-f11d-4437-b30e-b31de4282734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean along the first axis (axis=0)\n",
    "# To combine all individual run\n",
    "wg_pt_combined_matrix1 = np.mean(wg_pt_data_matrix1, axis=0)\n",
    "sg_pt_combined_matrix1 = np.mean(sg_pt_data_matrix1, axis=0)\n",
    "wg_pt_combined_matrix2 = np.mean(wg_pt_data_matrix2, axis=0)\n",
    "sg_pt_combined_matrix2 = np.mean(sg_pt_data_matrix2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e227dd8-b58c-4b73-a8c2-e60b4fbb68cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing diagonal values with small number for normalization\n",
    "np.fill_diagonal(wg_pt_combined_matrix1, 1e-9)\n",
    "np.fill_diagonal(wg_pt_combined_matrix2, 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23462ebf-9784-4023-b6a5-4ee3c77394da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.042474178867774\n",
      "40.00000005002191\n",
      "8.636569670430092\n",
      "40.00000013114292\n"
     ]
    }
   ],
   "source": [
    "# Get sum for normalization\n",
    "wg_pt_matrix_sum1 = pd.DataFrame(wg_pt_combined_matrix1).sum().sum()\n",
    "print(wg_pt_matrix_sum1)\n",
    "sg_pt_matrix_sum1 = pd.DataFrame(sg_pt_combined_matrix1).sum().sum()\n",
    "print(sg_pt_matrix_sum1)\n",
    "\n",
    "wg_pt_matrix_sum2 = pd.DataFrame(wg_pt_combined_matrix2).sum().sum()\n",
    "print(wg_pt_matrix_sum2)\n",
    "sg_pt_matrix_sum2 = pd.DataFrame(sg_pt_combined_matrix2).sum().sum()\n",
    "print(sg_pt_matrix_sum2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb3541f8-56f6-4400-bc56-f83550bbf13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Normalization\n",
    "wg_pt_mean_matrix_partial1 = wg_pt_combined_matrix1/wg_pt_matrix_sum1\n",
    "sg_pt_mean_matrix_partial1 = sg_pt_combined_matrix1/sg_pt_matrix_sum1\n",
    "\n",
    "wg_pt_mean_matrix_partial2 = wg_pt_combined_matrix2/wg_pt_matrix_sum2\n",
    "sg_pt_mean_matrix_partial2 = sg_pt_combined_matrix2/sg_pt_matrix_sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0de2c6de-6649-40ee-ba38-3a8b5d4a0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten for KL Divergence\n",
    "wg_pt_mean_matrix_partial_flatten1 = wg_pt_mean_matrix_partial1.flatten()\n",
    "sg_pt_mean_matrix_partial_flatten1 = sg_pt_mean_matrix_partial1.flatten()\n",
    "\n",
    "wg_pt_mean_matrix_partial_flatten2 = wg_pt_mean_matrix_partial2.flatten()\n",
    "sg_pt_mean_matrix_partial_flatten2 = sg_pt_mean_matrix_partial2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "731f9833-8cef-4581-8d86-0ee85d75ba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there is no 0, as KL divergence will be NAN\n",
    "wg_pt_mean_matrix_partial_flatten1[wg_pt_mean_matrix_partial_flatten1 < 1e-15] = 1e-12\n",
    "sg_pt_mean_matrix_partial_flatten1[sg_pt_mean_matrix_partial_flatten1 < 1e-15] = 1e-12\n",
    "\n",
    "wg_pt_mean_matrix_partial_flatten2[wg_pt_mean_matrix_partial_flatten2 < 1e-15] = 1e-12\n",
    "sg_pt_mean_matrix_partial_flatten2[sg_pt_mean_matrix_partial_flatten2 < 1e-15] = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e7b7efb-1985-401a-940e-8053e27e1838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence Naive: 0.8018927660501509\n",
      "KL Divergence Contrastive: 0.8093742643299559\n"
     ]
    }
   ],
   "source": [
    "# Calculate KL divergence - Standard Set Transformer\n",
    "kl_divergence_value_naive = np.sum([kl_divergence(p_row, q_row) for p_row, q_row in zip(wg_pt_mean_matrix_partial_flatten1, sg_pt_mean_matrix_partial_flatten1)])\n",
    "print(\"KL Divergence Naive:\", kl_divergence_value_naive)\n",
    "\n",
    "# Calculate KL divergence - Contrastive Pre-trained Model\n",
    "kl_divergence_value2_cont = np.sum([kl_divergence(p_row, q_row) for p_row, q_row in zip(wg_pt_mean_matrix_partial_flatten2, sg_pt_mean_matrix_partial_flatten2)])\n",
    "print(\"KL Divergence Contrastive:\", kl_divergence_value2_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddb2104a-1a59-4b48-b798-ebcfd0470c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall KL Divergence of Naive: 0.8018927640495719\n",
      "Overall KL Divergence of Contrastive: 0.8093742622572059\n"
     ]
    }
   ],
   "source": [
    "# Calculate KL divergence between the two distributions\n",
    "kl_divergence_array1 = entropy(wg_pt_mean_matrix_partial_flatten1, sg_pt_mean_matrix_partial_flatten1, base=2)\n",
    "\n",
    "kl_divergence_value1 = np.mean(kl_divergence_array1)  # or np.sum(kl_divergence_array)\n",
    "print(\"Overall KL Divergence of Naive:\", kl_divergence_value1)\n",
    "\n",
    "kl_divergence_array2 = entropy(wg_pt_mean_matrix_partial_flatten2, sg_pt_mean_matrix_partial_flatten2, base=2)\n",
    "\n",
    "kl_divergence_value2 = np.mean(kl_divergence_array2)  # or np.sum(kl_divergence_array)\n",
    "print(\"Overall KL Divergence of Contrastive:\", kl_divergence_value2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36804466-b1ab-4572-886c-1d81e9353170",
   "metadata": {},
   "source": [
    "# PCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8f04560-885d-48d7-bcb7-3c2a0bc1a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get probability distribution files for weak generalization - Standard set transformer\n",
    "wg_pt_path_pct1 = './strong_generalization/multi_class_probability_files/pct/ModelNet40/Weak_Generalization/ed256_0.2/'\n",
    "wg_pt_output_files_pct1 = [file for file in os.listdir(wg_pt_path_pct1) if file.startswith('output_file_') and file.endswith('.csv')]\n",
    "\n",
    "#Get probability distribution files for strong generalization - Standard set transformer\n",
    "sg_pt_path_pct1 = './strong_generalization/multi_class_probability_files/pct/ModelNet40/Strong_Generalization/ed256_0.2_bs32_ss256_vss128/'\n",
    "sg_pt_output_files_pct1 = [file for file in os.listdir(sg_pt_path_pct1) if file.startswith('output_file_') and file.endswith('.csv')]\n",
    "\n",
    "#Get probability distribution files for weak generalization - contrastive pre-trained model\n",
    "wg_pt_path_pct2 = './strong_generalization/multi_class_probability_files/pct/ModelNet40/Weak_Generalization/masked_encoder_160_512_150_200_pd1024_ed256_lr0.001_sndl_PCT_loss/'\n",
    "wg_pt_output_files_pct2 = [file for file in os.listdir(wg_pt_path_pct2) if file.startswith('output_file_') and file.endswith('.csv')]\n",
    "\n",
    "#Get probability distribution files for strong generalization - contrastive pre-trained model\n",
    "sg_pt_path_pct2 = './strong_generalization/multi_class_probability_files/pct/ModelNet40/Strong_Generalization/masked_encoder_160_512_150_200_pd1024_ed256_lr0.001_sndl_PCT_loss_pd1024_bs32_ss256_vss128/'\n",
    "sg_pt_output_files_pct2 = [file for file in os.listdir(sg_pt_path_pct2) if file.startswith('output_file_') and file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35fd9c59-b667-4fc3-9aa8-72d131eeb20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wg_pt_df_dict_pct1 = concat(wg_pt_path_pct1, wg_pt_output_files_pct1)\n",
    "sg_pt_df_dict_pct1 = concat(sg_pt_path_pct1, sg_pt_output_files_pct1)\n",
    "wg_pt_df_dict_pct2 = concat(wg_pt_path_pct2, wg_pt_output_files_pct2)\n",
    "sg_pt_df_dict_pct2 = concat(sg_pt_path_pct2, sg_pt_output_files_pct2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c97973a5-559b-43f5-be6d-0c777fc1969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D matrix from the DataFrames\n",
    "wg_pt_data_matrix_pct1 = np.array([df1.values for df1 in wg_pt_df_dict_pct1.values()])\n",
    "sg_pt_data_matrix_pct1 = np.array([df1.values for df1 in sg_pt_df_dict_pct1.values()])\n",
    "wg_pt_data_matrix_pct2 = np.array([df1.values for df1 in wg_pt_df_dict_pct2.values()])\n",
    "sg_pt_data_matrix_pct2 = np.array([df1.values for df1 in sg_pt_df_dict_pct2.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1670992-582e-46a6-9506-f23164467106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 40, 40)\n",
      "(10, 40, 40)\n",
      "(10, 40, 40)\n",
      "(10, 40, 40)\n"
     ]
    }
   ],
   "source": [
    "print(wg_pt_data_matrix_pct1.shape)\n",
    "print(sg_pt_data_matrix_pct1.shape)\n",
    "print(wg_pt_data_matrix_pct2.shape)\n",
    "print(sg_pt_data_matrix_pct2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c161569-2d5b-439e-8a59-26fe1fd475c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean along the first axis (axis=0)\n",
    "# To combine all individual run\n",
    "wg_pt_combined_matrix_pct1 = np.mean(wg_pt_data_matrix_pct1, axis=0)\n",
    "sg_pt_combined_matrix_pct1 = np.mean(sg_pt_data_matrix_pct1, axis=0)\n",
    "wg_pt_combined_matrix_pct2 = np.mean(wg_pt_data_matrix_pct2, axis=0)\n",
    "sg_pt_combined_matrix_pct2 = np.mean(sg_pt_data_matrix_pct2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85abced3-3714-4cc2-bd7f-7f5334c453f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg_pt_combined_matrix_pct1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "190d7af7-e9ad-4bb1-96be-f808bceb82e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing diagonal values with small number for normalization\n",
    "np.fill_diagonal(wg_pt_combined_matrix_pct1, 1e-9)\n",
    "np.fill_diagonal(wg_pt_combined_matrix_pct2, 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48f349be-33af-4cf8-9b31-99365b681c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.4093553364625535\n",
      "40.00000001229662\n",
      "8.322018361739964\n",
      "39.99999999075611\n"
     ]
    }
   ],
   "source": [
    "# Get sum for normalization\n",
    "wg_pt_matrix_sum_pct1 = pd.DataFrame(wg_pt_combined_matrix_pct1).sum().sum()\n",
    "print(wg_pt_matrix_sum_pct1)\n",
    "sg_pt_matrix_sum_pct1 = pd.DataFrame(sg_pt_combined_matrix_pct1).sum().sum()\n",
    "print(sg_pt_matrix_sum_pct1)\n",
    "\n",
    "wg_pt_matrix_sum_pct2 = pd.DataFrame(wg_pt_combined_matrix_pct2).sum().sum()\n",
    "print(wg_pt_matrix_sum_pct2)\n",
    "sg_pt_matrix_sum_pct2 = pd.DataFrame(sg_pt_combined_matrix_pct2).sum().sum()\n",
    "print(sg_pt_matrix_sum_pct2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43fb8227-d13e-4b9b-a962-2ad3638a79af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Normalization\n",
    "wg_pt_mean_matrix_partial_pct1 = wg_pt_combined_matrix_pct1/wg_pt_matrix_sum_pct1\n",
    "sg_pt_mean_matrix_partial_pct1 = sg_pt_combined_matrix_pct1/sg_pt_matrix_sum_pct1\n",
    "\n",
    "wg_pt_mean_matrix_partial_pct2 = wg_pt_combined_matrix_pct2/wg_pt_matrix_sum_pct2\n",
    "sg_pt_mean_matrix_partial_pct2 = sg_pt_combined_matrix_pct2/sg_pt_matrix_sum_pct2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acaf4b9e-3e04-4985-8434-7990f3e56b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten for KL Divergence\n",
    "wg_pt_mean_matrix_partial_flatten_pct1 = wg_pt_mean_matrix_partial_pct1.flatten()\n",
    "sg_pt_mean_matrix_partial_flatten_pct1 = sg_pt_mean_matrix_partial_pct1.flatten()\n",
    "\n",
    "wg_pt_mean_matrix_partial_flatten_pct2 = wg_pt_mean_matrix_partial_pct2.flatten()\n",
    "sg_pt_mean_matrix_partial_flatten_pct2 = sg_pt_mean_matrix_partial_pct2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e96a3ffa-ac26-4d00-8f74-a878a2cf26f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there is no 0, as KL divergence will be NAN\n",
    "wg_pt_mean_matrix_partial_flatten_pct1[wg_pt_mean_matrix_partial_flatten_pct1 < 1e-15] = 1e-12\n",
    "sg_pt_mean_matrix_partial_flatten_pct1[sg_pt_mean_matrix_partial_flatten_pct1 < 1e-15] = 1e-12\n",
    "\n",
    "wg_pt_mean_matrix_partial_flatten_pct2[wg_pt_mean_matrix_partial_flatten_pct2 < 1e-15] = 1e-12\n",
    "sg_pt_mean_matrix_partial_flatten_pct2[sg_pt_mean_matrix_partial_flatten_pct2 < 1e-15] = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "858289fb-fceb-4593-875f-582bc0e20b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence Naive PCT: 1.477473745756666\n",
      "KL Divergence Contrastive PCT: 1.283625666232929\n"
     ]
    }
   ],
   "source": [
    "# Calculate KL divergence - Standard Set Transformer\n",
    "kl_divergence_value_pct1 = np.sum([kl_divergence(p_row, q_row) for p_row, q_row in zip(wg_pt_mean_matrix_partial_flatten_pct1, sg_pt_mean_matrix_partial_flatten_pct1)])\n",
    "print(\"KL Divergence Naive PCT:\", kl_divergence_value_pct1)\n",
    "\n",
    "# Calculate KL divergence - Contrastive Pre-trained Model\n",
    "kl_divergence_value_pct2 = np.sum([kl_divergence(p_row, q_row) for p_row, q_row in zip(wg_pt_mean_matrix_partial_flatten_pct2, sg_pt_mean_matrix_partial_flatten_pct2)])\n",
    "print(\"KL Divergence Contrastive PCT:\", kl_divergence_value_pct2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74321258-f03d-4e76-bbb5-36363d4fb651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall KL Divergence of Naive: 1.4774737427043938\n",
      "Overall KL Divergence of Contrastive: 1.2836256635970325\n"
     ]
    }
   ],
   "source": [
    "# Calculate KL divergence between the two distributions\n",
    "kl_divergence_array_pct1 = entropy(wg_pt_mean_matrix_partial_flatten_pct1, sg_pt_mean_matrix_partial_flatten_pct1, base=2)\n",
    "\n",
    "kl_divergence_value_pct1 = np.mean(kl_divergence_array_pct1)  # or np.sum(kl_divergence_array)\n",
    "print(\"Overall KL Divergence of Naive:\", kl_divergence_value_pct1)\n",
    "\n",
    "kl_divergence_array_pct2 = entropy(wg_pt_mean_matrix_partial_flatten_pct2, sg_pt_mean_matrix_partial_flatten_pct2, base=2)\n",
    "\n",
    "kl_divergence_value_pct2 = np.mean(kl_divergence_array_pct2)  # or np.sum(kl_divergence_array)\n",
    "print(\"Overall KL Divergence of Contrastive:\", kl_divergence_value_pct2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874b1a1-87f3-40c9-bb70-352af5436d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40779c1a-60fd-456f-ab10-fc86c086d060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
